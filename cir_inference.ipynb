{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637ff3af-d07c-4fd4-933f-a17abbdfb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CRYPTOGRAPHY_OPENSSL_NO_LEGACY'] = '1'\n",
    "\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "# !pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fromage import models\n",
    "from fromage import utils\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90daf773-d243-4c14-a852-59eac58b7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_caption(caption: str) -> str:\n",
    "    # Truncate at period.\n",
    "    trunc_index = caption.find('.') + 1\n",
    "    if trunc_index < 0:\n",
    "        trunc_index = caption.find('\\n') + 1\n",
    "    caption = caption[:trunc_index]\n",
    "    return caption\n",
    "\n",
    "def display_interleaved_outputs(model_outputs, one_img_per_ret=True):\n",
    "    for output in model_outputs:\n",
    "        if type(output) == str:\n",
    "            print(output)\n",
    "        elif type(output) == list:\n",
    "            if one_img_per_ret:\n",
    "                plt.figure(figsize=(3, 3))\n",
    "                plt.imshow(np.array(output[0]))\n",
    "            else:\n",
    "                fig, ax = plt.subplots(1, len(output), figsize=(3 * len(output), 3))\n",
    "                for i, image in enumerate(output):\n",
    "                    image = np.array(image)\n",
    "                    ax[i].imshow(image)\n",
    "                    ax[i].set_title(f'Retrieval #{i+1}')\n",
    "            plt.show()\n",
    "        elif type(output) == Image.Image:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(np.array(output))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def get_image_from_path(image_path: str):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert('RGB')\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aed48196-bdd4-4aa3-93b5-66c8a1388a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "from collections import namedtuple\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import OPTForCausalLM, GPT2Tokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from fromage.models import MCL\n",
    "\n",
    "def load_model(model_dir: str,Llama=False):\n",
    "  model_args_path = os.path.join(model_dir, 'model_args.json')\n",
    "  model_ckpt_path = os.path.join(model_dir, 'pretrained_ckpt_10.pth.tar')\n",
    "\n",
    "\n",
    "\n",
    "  if not os.path.exists(model_args_path):\n",
    "    raise ValueError(f'model_args.json does not exist in {model_dir}.')\n",
    "  if not os.path.exists(model_ckpt_path):\n",
    "    raise ValueError(f'pretrained_ckpt.pth.tar does not exist in {model_dir}.')\n",
    "\n",
    "\n",
    "  with open(model_args_path, 'r') as f:\n",
    "      model_kwargs = json.load(f)\n",
    "\n",
    "  LenRET = 1\n",
    "  if 'LenRET' in model_kwargs.keys():\n",
    "    LenRET = model_kwargs['LenRET']\n",
    "  # Initialize tokenizer.\n",
    "  if Llama:\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"fromage/llama/hugging-llama-2-7b\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "  else:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_kwargs['opt_version'],local_files_only=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  tokenizer.add_special_tokens({\"cls_token\": \"<|image|>\"})\n",
    "  if LenRET==1:\n",
    "    tokenizer.add_tokens('[RET]')\n",
    "    ret_token_idx = tokenizer('[RET]', add_special_tokens=False).input_ids\n",
    "    assert len(ret_token_idx) == 1, ret_token_idx\n",
    "    model_kwargs['retrieval_token_idx'] = ret_token_idx[0]\n",
    "  else:\n",
    "    model_kwargs['retrieval_token_idx'] = []\n",
    "    for i in range(LenRET):\n",
    "      RET_Token = f'[RET{i}]'\n",
    "      tokenizer.add_tokens(RET_Token)\n",
    "      ret_token_idx = tokenizer(f'[RET{i}]', add_special_tokens=False).input_ids\n",
    "      assert len(ret_token_idx) == 1, ret_token_idx\n",
    "      model_kwargs['retrieval_token_idx'].append(ret_token_idx[0])\n",
    "\n",
    "\n",
    "  model_kwargs['Llama']=Llama\n",
    "  args = namedtuple('args', model_kwargs)(**model_kwargs)\n",
    "\n",
    "  model = MCL(tokenizer, args)\n",
    "  model = model.eval()\n",
    "  model = model.bfloat16()\n",
    "  model = model.cuda()\n",
    "\n",
    "  # Load pretrained linear mappings and [RET] embeddings.\n",
    "  checkpoint = torch.load(model_ckpt_path)\n",
    "  model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "  with torch.no_grad():\n",
    "    if LenRET==1:\n",
    "      model.model.input_embeddings.weight[model.model.retrieval_token_idx, :].copy_(checkpoint['state_dict']['ret_input_embeddings.weight'].squeeze().cpu().detach())\n",
    "    else:\n",
    "      model.model.input_embeddings.weight[model.model.retrieval_token_idx[0]:model.model.retrieval_token_idx[-1]+1, :].copy_(checkpoint['state_dict']['ret_input_embeddings.weight'].squeeze().cpu().detach())\n",
    "  logit_scale = model.model.logit_scale.exp()\n",
    "\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "723adb7a-5e4e-4785-a881-59658ae6daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 4 visual tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bde8cb90a104355b4f6213c809f26b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Llama for the language model.\n",
      "Freezing the LM.\n",
      "Initializing embedding for the retrieval token [RET] (id = [32002, 32003, 32004, 32005, 32006]).\n",
      "Restoring pretrained weights for the visual model.\n",
      "Freezing the VM.\n"
     ]
    }
   ],
   "source": [
    "# Load model used in the paper.\n",
    "model_dir = './runs/icml_save/'\n",
    "\n",
    "model = load_model(model_dir,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6daa836b-1e77-4330-aba9-ce697473be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPProcessor, CLIPModel,CLIPTextModel,CLIPVisionModel\n",
    "# clip_tokenizer = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\",local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "795fb152-cd1f-40df-ac27-f6f3710fbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "def generate_for_cir(\n",
    "    model, prompts: List, num_words: int = 0, ret_scale_factor: float = 1.0, top_p: float = 1.0, temperature: float = 0.0,\n",
    "    max_num_rets: int = 1, attention_mask =None,if_mask=True):\n",
    "\n",
    "    input_embs = []\n",
    "    input_ids = []\n",
    "    all_visual_embs = []\n",
    "    add_bos = True\n",
    "\n",
    "    attention_mask = []\n",
    "    for i, p in enumerate(prompts):\n",
    "      if type(p) == Image.Image:\n",
    "        # Encode as image.\n",
    "        pixel_values = utils.get_pixel_values_for_model(model.model.feature_extractor, p)\n",
    "        pixel_values = pixel_values.to(device=model.model.logit_scale.device, dtype=model.model.logit_scale.dtype)\n",
    "        pixel_values = pixel_values[None, ...]\n",
    "\n",
    "        visual_embs = model.model.get_visual_embs(pixel_values, mode='cap')  # (1, n_visual_tokens, D)\n",
    "\n",
    "        input_embs.append(visual_embs)\n",
    "        all_visual_embs.append(visual_embs)\n",
    "        attention_mask.append(torch.ones(visual_embs.shape[:2], dtype=torch.int64).to(visual_embs.device))\n",
    "      elif type(p) == str:\n",
    "        text_ids = model.model.tokenizer(p, add_special_tokens=True, return_tensors=\"pt\").input_ids.to(model.model.logit_scale.device)\n",
    "\n",
    "        masks_tc = model.model.tokenizer(p, add_special_tokens=True, return_tensors=\"pt\").attention_mask.to(model.model.logit_scale.device)\n",
    "        if isinstance(model.model.retrieval_token_idx,list):\n",
    "            for i in range(text_ids.shape[-1]):\n",
    "              if text_ids[0][i] in model.model.retrieval_token_idx:\n",
    "                masks_tc[0][i]+=1\n",
    "        attention_mask.append(masks_tc)\n",
    "        if not add_bos:\n",
    "          # Remove <bos> tag.\n",
    "          text_ids = text_ids[:, 1:]\n",
    "          masks_tc = masks_tc[:, 1:]\n",
    "        else:\n",
    "          # Only add <bos> once.\n",
    "          add_bos = False\n",
    "\n",
    "        text_embs = model.model.input_embeddings(text_ids)  # (1, T, D)\n",
    "\n",
    "        input_embs.append(text_embs)\n",
    "        input_ids.append(text_ids)\n",
    "      else:\n",
    "        raise ValueError(f'Input prompts should be either PIL.Image.Image or str types, got {type(p)} instead.')\n",
    "    input_embs = torch.cat(input_embs, dim=1)\n",
    "    input_ids = torch.cat(input_ids, dim=1)\n",
    "    attention_mask = torch.cat(attention_mask,dim=1)\n",
    "    \n",
    "    if num_words == 0:\n",
    "      generated_ids = input_ids\n",
    "      if if_mask ==False:\n",
    "        attention_mask = None\n",
    "      outputs = model.model.lm(inputs_embeds=input_embs, use_cache=False, attention_mask=attention_mask,output_hidden_states=True)\n",
    "      # Map outputs to embeddings, so we can retrieve embeddings from the [RET] tokens.\n",
    "      out = []\n",
    "      for x, fc in zip(model.model.args.text_emb_layers, model.model.text_hidden_fcs):\n",
    "          out.append(fc(outputs.hidden_states[x]))\n",
    "      embeddings = torch.stack(out, dim=-1).sum(dim=-1)\n",
    "\n",
    "    elif num_words > 0:\n",
    "      generated_ids, generated_embeddings, _ = model.model.generate(input_embs, num_words,\n",
    "        temperature=temperature, top_p=top_p, ret_scale_factor=ret_scale_factor)\n",
    "      embeddings = generated_embeddings[-1][:, input_embs.shape[1]:]\n",
    "\n",
    "      # Truncate to newline.\n",
    "      newline_token_id = model.model.tokenizer('\\n', add_special_tokens=False).input_ids[0]\n",
    "      trunc_idx = 0\n",
    "      for j in range(generated_ids.shape[1]):\n",
    "        if generated_ids[0, j] == newline_token_id:\n",
    "          trunc_idx = j\n",
    "          break\n",
    "      if trunc_idx > 0:\n",
    "        generated_ids = generated_ids[:, :trunc_idx]\n",
    "        embeddings = embeddings[:, :trunc_idx]\n",
    "    else:\n",
    "      raise ValueError\n",
    "\n",
    "    return_outputs = []\n",
    "    # Find up to max_num_rets [RET] tokens, and their corresponding scores.\n",
    "    if isinstance(model.model.retrieval_token_idx,list):\n",
    "      all_ret_idx = [i for i, x in enumerate(generated_ids[0, :] == model.model.retrieval_token_idx[0]) if x][:max_num_rets]\n",
    "    else:\n",
    "      all_ret_idx = [i for i, x in enumerate(generated_ids[0, :] == model.model.retrieval_token_idx) if x][:max_num_rets]\n",
    "    seen_image_idx = []  # Avoid showing the same image multiple times.\n",
    "\n",
    "    last_ret_idx = 0\n",
    "    ret_emb_ori=None\n",
    "    if len(all_ret_idx) == 0:\n",
    "\n",
    "      caption = model.model.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "      return_outputs.append(utils.truncate_caption(caption))\n",
    "    else:\n",
    "      ret_emb_ori = embeddings[:, all_ret_idx[0]+4:, :]\n",
    "\n",
    "      ret_emb_ori = model.model.transformer_fusion(ret_emb_ori)\n",
    "\n",
    "      ret_emb = torch.mean(ret_emb_ori, dim=1) # (N,D)\n",
    "\n",
    "\n",
    "    return ret_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffbf1cf8-4345-4d2e-b8e7-d42ff6b57e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "circo_image_path = './data/unlabeled2017/'\n",
    "cirr_image_path = './data/'\n",
    "data_dir = './data/'\n",
    "\n",
    "embs_path = './features/clip_model_CIRCO_embeddings.pkl'\n",
    "\n",
    "with open(embs_path, 'rb') as wf:\n",
    "    train_embs_data = pkl.load(wf)\n",
    "    path_array=(train_embs_data['paths'])\n",
    "    emb_matrix=(train_embs_data['embeddings'])\n",
    "\n",
    "emb_matrix = torch.tensor(emb_matrix).cuda().float()\n",
    "emb_matrix = emb_matrix / emb_matrix.norm(dim=-1, keepdim=True)\n",
    "path_array = [path.split('/')[-1] for path in path_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e26538c-94c4-4200-9e1f-67c3cc167e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [01:11<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "makp  5 tensor(0.1630)\n",
      "makp  10 tensor(0.1662)\n",
      "makp  25 tensor(0.1731)\n",
      "makp  50 tensor(0.1760)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_visual_embs = []\n",
    "all_caption=[]\n",
    "all_outputs =[]\n",
    "ret_scale_factor=0\n",
    "\n",
    "image_path = circo_image_path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "\n",
    "with open('repos/CIRCO/annotations/val.json','r') as f:\n",
    "    CIRCO_val = json.load(f)\n",
    "\n",
    "ifdisplay = False\n",
    "\n",
    "Topk = {}\n",
    "for mapk in [5,10,25,50]:\n",
    "    Topk[mapk]=[]\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(CIRCO_val[:]):\n",
    "        reference_img_id, target_img_id, relative_caption, _, gt_img_ids, id_,_ = sample.values()\n",
    "        refer_image_path = image_path + str(reference_img_id).zfill(12) + '.jpg'\n",
    "        refer_idx = path_array.index(str(reference_img_id).zfill(12)+ '.jpg')\n",
    "\n",
    "        gt_indexs = []\n",
    "        gt_image_paths = []\n",
    "        for id_ in gt_img_ids:\n",
    "            gt_image_path = image_path + str(id_).zfill(12) + '.jpg'\n",
    "            gt_image_paths.append(gt_image_path)\n",
    "            index = path_array.index(str(id_).zfill(12)+ '.jpg')\n",
    "            gt_indexs.append(index)\n",
    "        \n",
    "        inp_image = get_image_from_path(refer_image_path)\n",
    "\n",
    "        inp_text = 'Q:'+ relative_caption+ \".\\nA:it becomes a photo of [RET0][RET1][RET2][RET3][RET4]\"\n",
    "\n",
    "        prompt = [inp_image, inp_text]\n",
    "        if ifdisplay:\n",
    "            display_interleaved_outputs(prompt)\n",
    "\n",
    "        ret_emb = generate_for_cir(model,prompt,if_mask=True)\n",
    "\n",
    "        \n",
    "        ret_emb/=ret_emb.norm(dim=-1,keepdim=True)\n",
    "        \n",
    "        model_outputs =  emb_matrix @ (ret_emb.float()).T\n",
    "        model_outputs[refer_idx]=-100\n",
    "\n",
    "        \n",
    "        for mapk in [5,10,25,50]:\n",
    "            value,indexs = model_outputs.squeeze().topk(mapk)\n",
    "            k=0\n",
    "            x=1\n",
    "            for index in indexs:\n",
    "                if index in gt_indexs:\n",
    "                    k+=float(1/x)\n",
    "                x+=1\n",
    "            Topk[mapk].append(k/min(len(gt_indexs),mapk))\n",
    "\n",
    "    for mapk in [5,10,25,50]:\n",
    "        print('makp ',mapk,torch.tensor(Topk[mapk]).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2618c223-7f3b-4f30-9d40-fb3761ad8b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [04:01<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#102\n",
    "\n",
    "# image_path = '/home/liwei/exp/data/unlabeled2017/'\n",
    "image_path = './data/unlabeled2017/'\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "with open('./repos/CIRCO/annotations/test.json','r') as f:\n",
    "    CIRCO_test = json.load(f)\n",
    "    \n",
    "ifdisplay = False\n",
    "\n",
    "Topk = {}\n",
    "for mapk in [5,10,25,50]:\n",
    "    Topk[mapk]=[]\n",
    "results_circo = {}\n",
    "\n",
    "\n",
    "all_caption = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for sample in tqdm(CIRCO_test[:]):\n",
    "        reference_img_id, relative_caption, shared_concept, id_ = sample.values()\n",
    "        refer_image_path = image_path + str(reference_img_id).zfill(12) + '.jpg'\n",
    "        refer_idx = path_array.index(str(reference_img_id).zfill(12) + '.jpg')\n",
    "\n",
    "        inp_image = get_image_from_path(refer_image_path)\n",
    "\n",
    "        inp_text = 'Q:'+ relative_caption + '.\\nA:it becomes a photo of[RET0][RET1][RET2][RET3][RET4]'\n",
    "\n",
    "        prompt = [inp_image, inp_text]\n",
    "        if ifdisplay:\n",
    "            display_interleaved_outputs(prompt)\n",
    "        ret_emb = generate_for_cir(model,prompt,if_mask=True)\n",
    "        ret_emb/=ret_emb.norm(dim=-1,keepdim=True)\n",
    "\n",
    "\n",
    "        model_outputs =  emb_matrix @ (ret_emb.float()).T\n",
    "        model_outputs[refer_idx]=-100\n",
    "\n",
    "\n",
    "        similarity_all = model_outputs\n",
    "        value,indexs = similarity_all.squeeze().topk(50)\n",
    "        results_circo[str(id_)]=[]\n",
    "        for index in indexs:\n",
    "            index_circo = int(path_array[index].split('/')[-1].split('.')[0])\n",
    "            results_circo[str(id_)].append(index_circo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a29af5b0-258c-4078-923d-c2fa55588e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/circo_results.json','w') as f:\n",
    "    json.dump(results_circo,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef614912-606f-41cf-b0a7-4243c10cf193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0776ffe9-c8bd-4f94-af5c-ca2cb44adcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4181/4181 [21:13<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6316670652953839\n",
      "0.2535278641473332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "with open('repos/CIRR/captions/cap.rc2.val.json','r') as f:\n",
    "    cirr_eval = json.load(f)\n",
    "with open('features/clip_model_NLVR2_embeddings.pkl', 'rb') as wf:\n",
    "    cirr_embs_data = pkl.load(wf)\n",
    "with open('repos/CIRR/image_splits/split.rc2.val.json','r') as f:\n",
    "    test1=json.load(f)\n",
    "embedding_cirr =cirr_embs_data['embeddings']\n",
    "path_array_cirr = list(cirr_embs_data['paths'])\n",
    "path_array_cirr = [path.split('/')[-1] for path in path_array_cirr]\n",
    "image_path = 'data/NLVR2/dev/'\n",
    "Topk = {}\n",
    "ifdisplay=False\n",
    "R1 = 0\n",
    "R1_all=0\n",
    "num_sample = 0\n",
    "embedding_cirr = torch.tensor(embedding_cirr).cuda()\n",
    "embedding_cirr /= embedding_cirr.norm(dim=-1,keepdim=True)\n",
    "\n",
    "all_gallery_indexs = [path_array_cirr.index(str(gallery_img_id) + '.png') for gallery_img_id in test1.keys()]\n",
    "all_gallery_embeddings = embedding_cirr[all_gallery_indexs]\n",
    "fusion=False\n",
    "all_caption=[]\n",
    "with torch.no_grad():\n",
    "\n",
    "    for sample in tqdm(cirr_eval[:]):\n",
    "        num_sample+=1\n",
    "        reference_img_id = sample['reference']\n",
    "        target_img_id = sample['target_hard']\n",
    "        relative_caption = sample['caption']\n",
    "        gallery_img_ids = sample['img_set']['members'].copy()\n",
    "        gallery_img_ids.remove(reference_img_id)\n",
    "        gt_rank = gallery_img_ids.index(sample['target_hard'])\n",
    "\n",
    "        refer_image_path = image_path + str(reference_img_id) + '.png'\n",
    "        refer_idx = path_array_cirr.index(str(reference_img_id) + '.png')\n",
    "\n",
    "\n",
    "        gt_image_path = image_path + str(target_img_id)+ '.png'\n",
    "        gt_indexs = path_array_cirr.index(str(target_img_id)+ '.png')\n",
    "        gt_rank_all = all_gallery_indexs.index(gt_indexs)\n",
    "        refer_index_all = all_gallery_indexs.index(refer_idx)\n",
    "        gallery_indexs = [path_array_cirr.index(str(gallery_img_id) + '.png') for gallery_img_id in gallery_img_ids]\n",
    "\n",
    "        inp_image = get_image_from_path(refer_image_path)\n",
    "        \n",
    "            \n",
    "        inp_text = \"Q:\"+ relative_caption.lower()+\".\\nA:it becomes a photo of [RET0][RET1][RET2][RET3][RET4]\"\n",
    "        prompt = [inp_image,inp_text]\n",
    "        if ifdisplay:\n",
    "            display_interleaved_outputs(prompt)\n",
    "\n",
    "\n",
    "        emb= generate_for_cir(model,prompt,if_mask=True)\n",
    "\n",
    "        emb/=emb.norm(dim=-1,keepdim=True)\n",
    "\n",
    "        similarity = embedding_cirr[gallery_indexs]@emb.float().T\n",
    "        _,top_index = similarity.squeeze().topk(3)\n",
    "        if similarity.argmax()==gt_rank:\n",
    "            R1+=1\n",
    "\n",
    "        similarity_all = all_gallery_embeddings@emb.float().T\n",
    "        _,top_index = similarity_all.squeeze().topk(3)\n",
    "        similarity_all[refer_index_all]=-100\n",
    "        if similarity_all.argmax()==gt_rank_all:\n",
    "            R1_all+=1\n",
    "\n",
    "    \n",
    "\n",
    "print(R1/num_sample)\n",
    "print(R1_all/num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1da5c3cd-dd5a-4c7b-9a15-d122af7d6c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1fca6a81-2c36-43fb-87a4-7cb7eec82997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4148/4148 [21:11<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cirr_results/results_subset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 74\u001b[0m\n\u001b[1;32m     69\u001b[0m             results_all[pairid]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(test1\u001b[38;5;241m.\u001b[39mkeys())[index])\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(R1\u001b[38;5;241m/\u001b[39mnum_sample)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcirr_results/results_subset.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     75\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(results_subset,f)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcirr_results/results_all.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cirr_results/results_subset.json'"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open('repos/CIRR/captions/cap.rc2.test1.json','r') as f:\n",
    "    cirr_test1 = json.load(f)\n",
    "with open('features/clip_model_NLVR2_test1_embeddings.pkl', 'rb') as wf:\n",
    "    cirr_embs_data = pkl.load(wf)\n",
    "with open('repos/CIRR/image_splits/split.rc2.test1.json','r') as f:\n",
    "    test1=json.load(f)\n",
    "    \n",
    "embedding_cirr = cirr_embs_data['embeddings']\n",
    "path_array_cirr = cirr_embs_data['paths']\n",
    "\n",
    "\n",
    "image_path = 'data/NLVR2/test1/'\n",
    "results_subset = {\"version\":\"rc2\",\"metric\":\"recall_subset\"}\n",
    "\n",
    "results_all ={\"version\":\"rc2\",\"metric\":\"recall\"}\n",
    "\n",
    "\n",
    "\n",
    "Topk = {}\n",
    "ifdisplay=False\n",
    "R1 = 0\n",
    "num_sample = 0\n",
    "embedding_cirr = torch.tensor(embedding_cirr).cuda()\n",
    "embedding_cirr /= embedding_cirr.norm(dim=-1,keepdim=True)\n",
    "\n",
    "all_gallery_indexs = [path_array_cirr.index(str(gallery_img_id) + '.png') for gallery_img_id in test1.keys()]\n",
    "fusion=False\n",
    "all_caption=[]\n",
    "with torch.no_grad():\n",
    "\n",
    "    all_gallery_embeddings = embedding_cirr[all_gallery_indexs]\n",
    "    for sample in tqdm(cirr_test1[:]):\n",
    "        num_sample+=1\n",
    "        pairid = str(sample['pairid'])\n",
    "        reference_img_id = sample['reference']\n",
    "        relative_caption = sample['caption']\n",
    "        gallery_img_ids = sample['img_set']['members'].copy()\n",
    "        gallery_img_ids.remove(reference_img_id)\n",
    "\n",
    "        refer_image_path = image_path + str(reference_img_id) + '.png'\n",
    "        refer_idx = path_array_cirr.index(str(reference_img_id) + '.png')\n",
    " \n",
    "        refer_index_all = all_gallery_indexs.index(refer_idx)\n",
    "\n",
    "        gallery_indexs = [path_array_cirr.index(str(gallery_img_id) + '.png') for gallery_img_id in gallery_img_ids]\n",
    "\n",
    "        inp_image = get_image_from_path(refer_image_path)\n",
    "\n",
    "        inp_text = 'Q:'+ relative_caption.lower()+'.\\nA:it becomes a photo of [RET0][RET1][RET2][RET3][RET4]'\n",
    "        prompt = [inp_image, inp_text]\n",
    "        if ifdisplay:\n",
    "            display_interleaved_outputs(prompt)\n",
    "\n",
    "        emb= generate_for_images_and_texts_circo(model,prompt,if_mask=True)\n",
    "\n",
    "        emb/=emb.norm(dim=-1,keepdim=True)\n",
    "\n",
    "        similarity = embedding_cirr[gallery_indexs]@emb.float().T\n",
    "        value,indexs = similarity.squeeze().topk(3)\n",
    "        results_subset[pairid]=[]\n",
    "        for index in indexs:\n",
    "            results_subset[pairid].append(gallery_img_ids[index])\n",
    "        similarity_all = all_gallery_embeddings@emb.float().T\n",
    "        similarity_all[refer_index_all]=-100\n",
    "        value,indexs = similarity_all.squeeze().topk(50)\n",
    "        results_all[pairid]=[]\n",
    "        for index in indexs:\n",
    "            results_all[pairid].append(list(test1.keys())[index])\n",
    "\n",
    "\n",
    "print(R1/num_sample)\n",
    "\n",
    "with open('results/results_subset.json','w') as f:\n",
    "    json.dump(results_subset,f)\n",
    "with open('results/results_all.json','w') as f:\n",
    "    json.dump(results_all,f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
